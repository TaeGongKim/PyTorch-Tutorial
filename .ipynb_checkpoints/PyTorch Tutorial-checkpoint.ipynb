{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch로 딥러닝하기 : 60분만에 끝장내기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference : https://tutorials.pytorch.kr/beginner/deep_learning_60min_blitz.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial의 목표\n",
    "- PyTorch의 Tensor Library와 신경망을 이해한다.\n",
    "- 이미지를 분류하는 작은 신경망을 학습시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch==1.8.1\n",
      "torchvision==0.9.1\n",
      "torchvision==0.9.1\n"
     ]
    }
   ],
   "source": [
    "# version check\n",
    "!pip freeze | findstr torch\n",
    "!pip freeze | findstr torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install torch, torchvision\n",
    "# !pip install torch\n",
    "# !pip install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PyTorch가 무엇인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python 기반의 연산 패키지로 다음 두 상황을 대상으로 한다.\n",
    "- numpy를 대체하면서 GPU를 이용한 연산이 필요한 경우\n",
    "- 최대한의 유연성과 속도를 제공하는 딥러닝 연구 플랫폼이 필요한 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Tensors\n",
    "- Tensor는 Numpy의 ndarray와 유사하며, GPU를 사용한 \"연산 가속\"도 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.9349e-19, 4.5445e+30, 4.7429e+30],\n",
      "        [3.0570e+32, 1.8469e+25, 8.7126e-04],\n",
      "        [7.3154e+34, 1.7288e+28, 3.2952e-15],\n",
      "        [7.3988e+31, 4.4849e+21, 2.7370e+20],\n",
      "        [6.4640e-04, 7.9290e+29, 7.5546e+31]])\n"
     ]
    }
   ],
   "source": [
    "# 초기화되지 않은 행렬을 생성\n",
    "x = torch.empty(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5670, 0.0493, 0.9731],\n",
      "        [0.7960, 0.1943, 0.0140],\n",
      "        [0.3885, 0.0652, 0.6272],\n",
      "        [0.1484, 0.3374, 0.8849],\n",
      "        [0.8038, 0.0625, 0.6505]])\n"
     ]
    }
   ],
   "source": [
    "# 무작위로 초기화된 행렬을 생성\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "# type : long, value : 0 로 채워진 행렬을 생성\n",
    "x = torch.zeros(5, 3, dtype = torch.long)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000])\n"
     ]
    }
   ],
   "source": [
    "# data를 직접 넣어 tensor를 생성\n",
    "x = torch.tensor([5.5, 3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# 기존 tensor를 바탕으로 새로운 tensor를 생성\n",
    "# 새로운 값을 제공받지 않는 한, 입력 tensor의 속성(dtype, ...)들을 재사용한다.\n",
    "\n",
    "# new_* : 크기를 입력으로 받는다.\n",
    "x = x.new_ones(5, 3, dtype = torch.double)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4121, -0.1865,  0.3836],\n",
      "        [-0.1929, -0.6331,  0.6119],\n",
      "        [-0.3199, -0.9287,  0.7241],\n",
      "        [ 0.0665,  0.9417, -1.3038],\n",
      "        [-0.4469, -2.1109,  1.5115]])\n"
     ]
    }
   ],
   "source": [
    "# dtype은 Override 한다.\n",
    "# 결과는 동일한 크기를 갖는다.\n",
    "x = torch.randn_like(x, dtype = torch.float)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "# 행렬의 size print\n",
    "# size type은 tuple, 모든 tuple 연산을 지원함\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 연산 (Operations)\n",
    "연산을 위한 여러가지 문법을 제공한다.\n",
    "- 덧셈\n",
    "- 인덱싱\n",
    "- 크기 변경  \n",
    "\n",
    "다른 연산 문법은 다음 https://pytorch.org/docs/stable/torch.html 참조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 덧셈 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2992, 1.0927, 1.3163],\n",
      "        [1.4578, 1.3460, 0.8640],\n",
      "        [1.1502, 1.0358, 0.6358],\n",
      "        [0.9516, 1.0418, 1.1592],\n",
      "        [0.2290, 1.4459, 1.4041]])\n"
     ]
    }
   ],
   "source": [
    "# 덧셈 연산 ver 1\n",
    "x = torch.rand(5, 3)\n",
    "y = torch.rand(5, 3)\n",
    "\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2992, 1.0927, 1.3163],\n",
      "        [1.4578, 1.3460, 0.8640],\n",
      "        [1.1502, 1.0358, 0.6358],\n",
      "        [0.9516, 1.0418, 1.1592],\n",
      "        [0.2290, 1.4459, 1.4041]])\n"
     ]
    }
   ],
   "source": [
    "# 덧셈 연산 ver 2\n",
    "print(torch.add(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2992, 1.0927, 1.3163],\n",
      "        [1.4578, 1.3460, 0.8640],\n",
      "        [1.1502, 1.0358, 0.6358],\n",
      "        [0.9516, 1.0418, 1.1592],\n",
      "        [0.2290, 1.4459, 1.4041]])\n"
     ]
    }
   ],
   "source": [
    "# 덧셈 결과는 tensor 인자로 제공\n",
    "result = torch.empty(5, 3)\n",
    "torch.add(x, y, out = result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2992, 1.0927, 1.3163],\n",
      "        [1.4578, 1.3460, 0.8640],\n",
      "        [1.1502, 1.0358, 0.6358],\n",
      "        [0.9516, 1.0418, 1.1592],\n",
      "        [0.2290, 1.4459, 1.4041]])\n"
     ]
    }
   ],
   "source": [
    "# 덧셈 in-place\n",
    "# in-place 방식은 메소드 뒤에 '_' 가 붙는다.\n",
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tensor인덱싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3181, 0.5167, 0.7828, 0.2496, 0.8877])\n"
     ]
    }
   ],
   "source": [
    "# numpy 처럼 인덱싱 표기 방법 사용 가능!!\n",
    "print(x[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tensor size 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "# tensor 크기 변경\n",
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8) # -1은 다른 차원에서 유추!!\n",
    "\n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0107])\n",
      "-0.010661723092198372\n"
     ]
    }
   ],
   "source": [
    "# 하나의 값을 가지는 tensor의 경우 .item() 메소드를 이용하여 하나의 숫자 값을 얻을 수 있다.\n",
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Torch Tensor -> Numpy Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "b = a.numpy()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "# Torch Tensor가 CPU 상에 있다면\n",
    "# Torch Tensor와 Numpy Array가 메모리 공간을 공유하기 때문에,\n",
    "# 하나를 변경하면 다른 하나도 변경 됨\n",
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Numpy Array -> Torch Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "np.add(a, 1, out = a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 CUDA Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .to 메소드를 사용하여 Tensor를 어떠한 장치로도 이동 가능!!!\n",
    "\n",
    "## 'torch.device' 를 사용하여 tensort를 GPU 안팎으로 이동\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    y = torch.ones_like(x, device = device)\n",
    "    x = x.to(device)\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to('cpu', torch.double))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2. AUTOGRAD : 자동 미분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PyTorch의 모든 신경망의 중심에는 autograd 패키지가 있다.\n",
    "- autograd 패키지는 Tensor의 모든 연산에 대해 자동 미분을 제공함\n",
    "- define-by-run 프레임워크 : 코드를 어떻게 작성하여 실행하느냐에 따라 역전파가 정의\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tensor의 .requires_grad 속성을 True로 설정 하면 해당 tensor에서 이뤄진 모든 연산들을 추적한다.\n",
    "- 계산 완료 후 .backward()를 호출하면 모든 gradient를 자동으로 계산\n",
    "- 해당 tensor의 gradient는 .grad 속성에 누적\n",
    "- tensor의 기록 추적 중단하려면 .detach()를 호출하여 연산 기록으로부터 분리 (이후 연산들이 추적되는 것을 방지 가능)\n",
    "- 기록을 추적하는 것을 방지하기 위해 with torch.no_grad(): 로 코드를 감쌀 수 있다. -> gradient는 필요없지만, requires_grad=True가 설정 되어 학습 가능한 매개변수를 갖는 모델을 평가할 때 유용하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Autograd 구현 시 Function 클래스의 중요도가 높다.\n",
    "- Tensor와 Function은 서로 연결되어 있음\n",
    "- 모든 연산 과정을 부호화하여 순환하지 않는 그래프를 생성 (acyclic graph)\n",
    "- 각 tensor는 .grad_fn 속성을 가지고 있음, 이는 tensor를 생성한 function을 참조하고 있다.\n",
    "- 단, 사용자가 만든 Tensor는 예외 -> 이때 grad_fn은 None으로 설정 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# tensor의 속성 requires_grad = True 로 설정하여 연산을 기록한다.\n",
    "x = torch.ones(2, 2, requires_grad = True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# add 연산 수행\n",
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x0000029DF55CBC40>\n"
     ]
    }
   ],
   "source": [
    "# 연산의 결과로 생성된 tensor이기 때문에 grad_fn을 갖는다.\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# 사용자가 정의한 tensor이기 때문에 grad_fn은 None 값을 가진다.\n",
    "print(x.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# y tensor에 다른 연산 수행\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x0000029DF14F2AC0>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# requires_grad_(...) 는 기존 tensor의 requires_grad 값을 in-place 하여 변경한다.\n",
    "# 입력값이 지정되지 않으면 기본 값은 False 이다.\n",
    "a = torch.randn(2, 2)\n",
    "a = ((a *3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)\n",
    "print(a.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out : scalar value\n",
    "# out.backward() == out.backward(torch.tensor(1.))\n",
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.5 로 이루어진 행렬을 확인할 수 있습니다. out 을 Tensor “o” 라고 하면, 다음과 같이 구할 수 있습니다. o=14∑izi 이고, zi=3(xi+2)2 이므로 zi∣∣xi=1=27 입니다. 따라서, ∂o∂xi=32(xi+2) 이므로, ∂o∂xi∣∣xi=1=92=4.5 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# out = y * y * 3\n",
    "# y = x + 2\n",
    "# x = [[1, 1],\n",
    "#      [1, 1]]\n",
    "# 즉, out = 3(x+2)^2\n",
    "# dout / dx = 6(x+2)\n",
    "print(x)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1102.3069, -1487.0493,    -5.5580], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad = True)\n",
    "y = x * 2\n",
    "\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "    \n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.1200e+01, 5.1200e+02, 5.1200e-02])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(v)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "y = x.detach()\n",
    "print(y.requires_grad)\n",
    "print(x.eq(y).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
